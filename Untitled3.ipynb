{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMQgJ8KzR1VlGZ2RsUnlGee",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pfaendler/ETH_SysBio_Exercises/blob/main/Untitled3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ROC and PR curves - Introduction{ display-mode: \"form\" }\n",
        "#@markdown Developments in machine learning have enabled biologists to build models for various classification tasks (e.g. WT mouse vs. KO mouse). In this exercise we are going to learn how to assess the perfomance of a model (how good or how bad it is) using different metrics including \"Receiver Operating Characteristic\" (ROC) or \"Precision Recall\" (PR) curves. \n",
        "\n",
        "#@markdown To do so, we first need to define some terms, which will help us to understand these details behind these metrics.\n",
        "#@markdown Commonly, we have an actual value that we would like to predict with our model. This will result in 4 different possibilities:\n",
        "\n",
        "#@markdown - True Positives (TP) - both actual value and prediction are 'true' (e.g. WT mouse predicted to be WT)\n",
        "#@markdown - False Positives (FP) - actual value is 'false' and prediction is 'true' (e.g. KO mouse predicted to be WT)\n",
        "#@markdown - False Negatives (FN) - actual value is 'true' and prediction is 'false' (e.g. WT mouse predicted to be KO)\n",
        "#@markdown - True Negatives (TN) - both actual value and prediction are 'false' (e.g. KO mouse predicted to be KO)\n",
        "\n",
        "#@markdown These resulting values can be \n",
        "\n",
        "\n",
        "#@markdown <img src='https://cdn.analyticsvidhya.com/wp-content/uploads/2020/04/Basic-Confusion-matrix.png' width=250px><br> \n",
        "#@markdown <em>**Figure 1:** Schematic confusion matrix <br> (copied from https://www.analyticsvidhya.com/blog/2020/06/auc-roc-curve-machine-learning/) </em>\n",
        "\n",
        "#@markdown - Specificity = $TN \\over TN + FP $\n",
        "#@markdown - Precision = $TP \\over TP + FP $\n",
        "#@markdown - Sensitivity = Recall = $TP \\over TP + FN $\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SHK3Hd7eYylH"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ROC and PR for Krug et al. (2020)\n",
        "#@markdown Set up the environment, and download the raw data from ([Krug et al., 2020](https://doi.org/10.1016/j.cell.2020.10.036)) used throughout the praktikum:\n"
      ],
      "metadata": {
        "id": "j8uoNWf8Y9kI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}